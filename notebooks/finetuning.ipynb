{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting accelerate\n","  Downloading accelerate-0.30.1-py3-none-any.whl.metadata (18 kB)\n","Collecting peft\n","  Downloading peft-0.11.1-py3-none-any.whl.metadata (13 kB)\n","Collecting bitsandbytes\n","  Downloading bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl.metadata (2.2 kB)\n","Collecting transformers\n","  Downloading transformers-4.41.1-py3-none-any.whl.metadata (43 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.8/43.8 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting trl\n","  Downloading trl-0.8.6-py3-none-any.whl.metadata (11 kB)\n","Requirement already satisfied: numpy>=1.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from accelerate) (1.26.2)\n","Requirement already satisfied: packaging>=20.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from accelerate) (23.2)\n","Requirement already satisfied: psutil in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from accelerate) (5.9.8)\n","Requirement already satisfied: pyyaml in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from accelerate) (6.0.1)\n","Requirement already satisfied: torch>=1.10.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from accelerate) (2.2.1+cu121)\n","Collecting huggingface-hub (from accelerate)\n","  Downloading huggingface_hub-0.23.1-py3-none-any.whl.metadata (12 kB)\n","Collecting safetensors>=0.3.1 (from accelerate)\n","  Downloading safetensors-0.4.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n","Requirement already satisfied: tqdm in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from peft) (4.66.4)\n","Requirement already satisfied: filelock in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (3.14.0)\n","Collecting regex!=2019.12.17 (from transformers)\n","  Downloading regex-2024.5.15-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.9/40.9 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: requests in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (2.32.2)\n","Collecting tokenizers<0.20,>=0.19 (from transformers)\n","  Downloading tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n","Collecting datasets (from trl)\n","  Downloading datasets-2.19.1-py3-none-any.whl.metadata (19 kB)\n","Collecting tyro>=0.5.11 (from trl)\n","  Downloading tyro-0.8.4-py3-none-any.whl.metadata (7.9 kB)\n","Requirement already satisfied: fsspec>=2023.5.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from huggingface-hub->accelerate) (2024.5.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from huggingface-hub->accelerate) (4.11.0)\n","Requirement already satisfied: sympy in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.12)\n","Requirement already satisfied: networkx in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.3)\n","Requirement already satisfied: jinja2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.4)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n","Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (8.9.2.26)\n","Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.1.3.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (11.0.2.54)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (10.3.2.106)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (11.4.5.107)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.1.0.106)\n","Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (2.19.3)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n","Requirement already satisfied: triton==2.2.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (2.2.0)\n","Requirement already satisfied: nvidia-nvjitlink-cu12 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate) (12.5.40)\n","Collecting docstring-parser>=0.14.1 (from tyro>=0.5.11->trl)\n","  Downloading docstring_parser-0.16-py3-none-any.whl.metadata (3.0 kB)\n","Requirement already satisfied: rich>=11.1.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from tyro>=0.5.11->trl) (13.7.1)\n","Collecting shtab>=1.5.6 (from tyro>=0.5.11->trl)\n","  Downloading shtab-1.7.1-py3-none-any.whl.metadata (7.3 kB)\n","Requirement already satisfied: pyarrow>=12.0.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets->trl) (16.1.0)\n","Collecting pyarrow-hotfix (from datasets->trl)\n","  Downloading pyarrow_hotfix-0.6-py3-none-any.whl.metadata (3.6 kB)\n","Collecting dill<0.3.9,>=0.3.0 (from datasets->trl)\n","  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n","Requirement already satisfied: pandas in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets->trl) (2.1.4)\n","Collecting xxhash (from datasets->trl)\n","  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n","Collecting multiprocess (from datasets->trl)\n","  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n","Collecting fsspec>=2023.5.0 (from huggingface-hub->accelerate)\n","  Downloading fsspec-2024.3.1-py3-none-any.whl.metadata (6.8 kB)\n","Requirement already satisfied: aiohttp in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets->trl) (3.9.5)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->transformers) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->transformers) (2.2.1)\n","Requirement already satisfied: certifi>=2017.4.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->transformers) (2024.2.2)\n","Requirement already satisfied: aiosignal>=1.1.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets->trl) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets->trl) (23.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets->trl) (1.4.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets->trl) (6.0.5)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets->trl) (1.9.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets->trl) (4.0.3)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl) (2.18.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pandas->datasets->trl) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pandas->datasets->trl) (2024.1)\n","Requirement already satisfied: tzdata>=2022.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pandas->datasets->trl) (2024.1)\n","Requirement already satisfied: mpmath>=0.19 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n","Requirement already satisfied: mdurl~=0.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro>=0.5.11->trl) (0.1.2)\n","Requirement already satisfied: six>=1.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets->trl) (1.16.0)\n","Downloading accelerate-0.30.1-py3-none-any.whl (302 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.6/302.6 kB\u001b[0m \u001b[31m29.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading peft-0.11.1-py3-none-any.whl (251 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m251.6/251.6 kB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl (119.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.8/119.8 MB\u001b[0m \u001b[31m48.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hDownloading transformers-4.41.1-py3-none-any.whl (9.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.1/9.1 MB\u001b[0m \u001b[31m123.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hDownloading trl-0.8.6-py3-none-any.whl (245 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m245.2/245.2 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading huggingface_hub-0.23.1-py3-none-any.whl (401 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m401.3/401.3 kB\u001b[0m \u001b[31m36.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading regex-2024.5.15-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (775 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m775.1/775.1 kB\u001b[0m \u001b[31m54.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading safetensors-0.4.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m50.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m99.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n","\u001b[?25hDownloading tyro-0.8.4-py3-none-any.whl (102 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.4/102.4 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading datasets-2.19.1-py3-none-any.whl (542 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m542.0/542.0 kB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading docstring_parser-0.16-py3-none-any.whl (36 kB)\n","Downloading fsspec-2024.3.1-py3-none-any.whl (171 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m172.0/172.0 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading shtab-1.7.1-py3-none-any.whl (14 kB)\n","Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n","Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: xxhash, shtab, safetensors, regex, pyarrow-hotfix, fsspec, docstring-parser, dill, multiprocess, huggingface-hub, tyro, tokenizers, transformers, datasets, bitsandbytes, accelerate, trl, peft\n","  Attempting uninstall: fsspec\n","    Found existing installation: fsspec 2024.5.0\n","    Uninstalling fsspec-2024.5.0:\n","      Successfully uninstalled fsspec-2024.5.0\n","Successfully installed accelerate-0.30.1 bitsandbytes-0.43.1 datasets-2.19.1 dill-0.3.8 docstring-parser-0.16 fsspec-2024.3.1 huggingface-hub-0.23.1 multiprocess-0.70.16 peft-0.11.1 pyarrow-hotfix-0.6 regex-2024.5.15 safetensors-0.4.3 shtab-1.7.1 tokenizers-0.19.1 transformers-4.41.1 trl-0.8.6 tyro-0.8.4 xxhash-3.4.1\n"]}],"source":["! pip install accelerate peft bitsandbytes transformers trl"]},{"cell_type":"code","execution_count":2,"metadata":{"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9fec20ca80624b4a91af406b37542e2f","version_major":2,"version_minor":0},"text/plain":["Downloading readme:   0%|          | 0.00/1.91k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Repo card metadata block was not found. Setting CardData to empty.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"18a0fca82ab04fabbcc7bfc4b1709ce0","version_major":2,"version_minor":0},"text/plain":["Downloading data:   0%|          | 0.00/4.32M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"aa09fe8707964c7ba680888a6b334a3c","version_major":2,"version_minor":0},"text/plain":["Downloading data:   0%|          | 0.00/540k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d5ab454e6baa4811a82ea4c9c72c89cd","version_major":2,"version_minor":0},"text/plain":["Downloading data:   0%|          | 0.00/544k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c2a86e338fc74ee38d76b9c23e9f3bea","version_major":2,"version_minor":0},"text/plain":["Generating train split: 0 examples [00:00, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e256e9e76d6b4927b09ec5e0b382962f","version_major":2,"version_minor":0},"text/plain":["Generating validation split: 0 examples [00:00, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b3e34478d08744608718cd5e9cc5b431","version_major":2,"version_minor":0},"text/plain":["Generating test split: 0 examples [00:00, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3fd00da3163044849b91b403ca24bb6e","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/19658 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0b87c4394fe14e2fb077fbce784f0b45","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/749 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"39ce4d93906d4174a4b39db81045f4f4","version_major":2,"version_minor":0},"text/plain":["tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"18cca5c98ac64959b9048201eed0cfc5","version_major":2,"version_minor":0},"text/plain":["tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1b26fb13cd1c4411a16582e38fbd37f2","version_major":2,"version_minor":0},"text/plain":["special_tokens_map.json:   0%|          | 0.00/411 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"09108c5d282d41178343a5a1b91193cb","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/637 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7bda62591b914ef99740d0a76a900f44","version_major":2,"version_minor":0},"text/plain":["model.safetensors.index.json:   0%|          | 0.00/25.1k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d6124d69aefa49738ee55c6b0fba6dfd","version_major":2,"version_minor":0},"text/plain":["Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1623c0af91bc467ea17f6a2717fe8ec4","version_major":2,"version_minor":0},"text/plain":["model-00001-of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d5b789ea7db4498db2afed089f4fb47f","version_major":2,"version_minor":0},"text/plain":["model-00002-of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d5af48e1f0804aa5938655df20c286ba","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"effd4d7aea3d41a28aa22fd184bd3f52","version_major":2,"version_minor":0},"text/plain":["generation_config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"184d69588e5b4dd1b4c5a3f76f4227b3","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/19658 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:318: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.\n","  warnings.warn(\n","max_steps is given, it will override any value given in num_train_epochs\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [100/100 09:47, Epoch 0/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>10</td>\n","      <td>2.502200</td>\n","    </tr>\n","    <tr>\n","      <td>20</td>\n","      <td>1.595500</td>\n","    </tr>\n","    <tr>\n","      <td>30</td>\n","      <td>1.402400</td>\n","    </tr>\n","    <tr>\n","      <td>40</td>\n","      <td>1.336400</td>\n","    </tr>\n","    <tr>\n","      <td>50</td>\n","      <td>1.293400</td>\n","    </tr>\n","    <tr>\n","      <td>60</td>\n","      <td>1.307600</td>\n","    </tr>\n","    <tr>\n","      <td>70</td>\n","      <td>1.296200</td>\n","    </tr>\n","    <tr>\n","      <td>80</td>\n","      <td>1.255800</td>\n","    </tr>\n","    <tr>\n","      <td>90</td>\n","      <td>1.247400</td>\n","    </tr>\n","    <tr>\n","      <td>100</td>\n","      <td>1.283400</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n"]}],"source":["from datasets import load_dataset\n","from peft import LoraConfig, AutoPeftModelForCausalLM\n","from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments\n","from trl import SFTTrainer\n","\n","def preprocess_example(example):\n","    example_dict = {\n","        \"srno\": example.get(\"srno\", None),\n","        \"nl_command\": example.get(\"nl_command\", None),\n","        \"bash_code\": example.get(\"bash_code\", None),\n","    }\n","    text = f\"[INST] Docstring: {example_dict['nl_command']} [/INST] Code: {example_dict['bash_code']}\"\n","    return {\"text\": text}\n","\n","def finetune_llama_v2():\n","    # Load your dataset\n","    dataset = load_dataset(\"AnishJoshi/nl2bash-custom\")\n","\n","    # Preprocess the dataset\n","    data = dataset[\"train\"].map(preprocess_example, remove_columns=[\"srno\", \"nl_command\", \"bash_code\"])\n","\n","    tokenizer = AutoTokenizer.from_pretrained(\"codellama/CodeLlama-7b-hf\")\n","    tokenizer.pad_token = tokenizer.eos_token\n","\n","    bnb_config = BitsAndBytesConfig(\n","        load_in_4bit=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=\"float16\", bnb_4bit_use_double_quant=True\n","    )\n","\n","    model = AutoModelForCausalLM.from_pretrained(\n","        \"codellama/CodeLlama-7b-hf\", quantization_config=bnb_config, device_map=\"auto\"\n","    )\n","\n","    model.config.use_cache = False\n","    model.config.pretraining_tp = 1\n","\n","    peft_config = LoraConfig(\n","        r=8, lora_alpha=16, lora_dropout=0.05, bias=\"none\", task_type=\"CAUSAL_LM\"\n","    )\n","\n","    training_arguments = TrainingArguments(\n","        output_dir=\"codellama2-finetuned-nl2bash\",\n","        per_device_train_batch_size=2,\n","        gradient_accumulation_steps=16,\n","        optim=\"paged_adamw_32bit\",\n","        learning_rate=2e-4,\n","        lr_scheduler_type=\"cosine\",\n","        save_strategy=\"epoch\",\n","        logging_steps=10,\n","        num_train_epochs=1,\n","        max_steps=100,\n","        fp16=True,\n","        push_to_hub=False,\n","        report_to=\"none\"\n","    )\n","\n","    trainer = SFTTrainer(\n","        model=model,\n","        train_dataset=data,\n","        peft_config=peft_config,\n","        dataset_text_field=\"text\",\n","        args=training_arguments,\n","        tokenizer=tokenizer,\n","        packing=False,\n","        max_seq_length=512\n","    )\n","\n","    trainer.train()\n","    return model, tokenizer\n","\n","if __name__ == \"__main__\":\n","    ft_mode, og_tokenizer = finetune_llama_v2()"]},{"cell_type":"code","execution_count":3,"metadata":{"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:515: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","  warnings.warn(\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/generation/utils.py:1659: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Predicted Bash Command: [INST] Docstring: List all the processes that are using more than 100MB of memory and sort them by memory usage in descending order. [/INST] Code: ps -eo pid,user,cmd,rss | awk '$4 > 100000 {print $0}' | sort -k 4 -nr | head -n 10 | column -t | sed 's/^[ \\t]*//' | sed 's/[ \\t]*$//' | sed 's/^[ \\t]*//' | sed 's/[ \\t]*$//' | sed 's/^[ \\t]*//' | sed 's/[ \\t]*$//' | sed 's/^[ \\t]*//' | sed 's/[ \\t]*$//' | sed 's/^[ \\t]*//' | sed 's\n"]}],"source":["ft_model = ft_mode\n","def run_inference(input_text):\n","\n","    # Tokenize the user input\n","    input_ids = og_tokenizer(input_text, return_tensors=\"pt\").input_ids\n","\n","    # Feed the tokenized input into the model for inference\n","    output_ids = ft_model.generate(input_ids, max_length=200, num_return_sequences=1, temperature=0.7)\n","\n","    # Decode the output tokens to generate the predicted bash command\n","    output_text = og_tokenizer.decode(output_ids[0], skip_special_tokens=True)\n","\n","    return output_text\n","\n","if __name__ == \"__main__\":\n","    # Example usage\n","    nl_command = 'List all the processes that are using more than 100MB of memory and sort them by memory usage in descending order.'\n","\n","    # Prepare the input for the model\n","    input_text = f\"[INST] Docstring: {nl_command} [/INST] Code:\"\n","    predicted_bash_command = run_inference(input_text)\n","    print(\"Predicted Bash Command:\", predicted_bash_command)\n"]},{"cell_type":"code","execution_count":4,"metadata":{"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"]},{"name":"stdout","output_type":"stream","text":["Generated Bash Command:\n","Bash code to list all the processes that are using more than 100MB of memory and sort them by memory usage in descending order.\n","\n","\\begin{code}\n","ps -eo pid,user,pri,vsz,args | awk '$4>100000 {print $0}' | sort -k 4 -r\n","\\end{code}\n","\n","Comment: I'm not sure what you mean by \"sort them by memory usage in descending order\".  Do you mean \"sort them by memory usage in descending order of memory usage\"?  Or do you mean \"sort them by memory usage in descending order of memory usage, but only if memory usage\n"]}],"source":["# Define the natural language command\n","nl_command = 'Bash code to list all the processes that are using more than 100MB of memory and sort them by memory usage in descending order.'\n","\n","# Prepare the input for the model\n","input_text = f\"[INST] Docstring: {nl_command} [/INST] Code:\"\n","\n","# Tokenize the input\n","inputs = og_tokenizer(nl_command, return_tensors=\"pt\").to(ft_model.device)\n","\n","# Generate the output\n","outputs = ft_model.generate(**inputs, max_length=150)\n","\n","# Decode the output\n","generated_code = og_tokenizer.decode(outputs[0], skip_special_tokens=True)\n","\n","print(\"Generated Bash Command:\")\n","print(generated_code)\n"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.status.busy":"2024-05-25T23:44:01.248662Z","iopub.status.idle":"2024-05-25T23:44:01.249412Z","shell.execute_reply":"2024-05-25T23:44:01.249154Z","shell.execute_reply.started":"2024-05-25T23:44:01.249132Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Repo card metadata block was not found. Setting CardData to empty.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"924a308f3e0c42fcabf7021f76486371","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/2458 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"}],"source":["test_dataset = load_dataset(\"AnishJoshi/nl2bash-custom\", split=\"test\")\n","test_data = test_dataset.map(preprocess_example, remove_columns=[\"srno\", \"nl_command\", \"bash_code\"])"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-05-25T23:55:06.569425Z","iopub.status.busy":"2024-05-25T23:55:06.569125Z","iopub.status.idle":"2024-05-25T23:55:06.576048Z","shell.execute_reply":"2024-05-25T23:55:06.575070Z","shell.execute_reply.started":"2024-05-25T23:55:06.569392Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["nl_command = Find all .mp3 files with more then 10MB and delete them from root directory .\n","code = find /  -type f -name *.mp3 -size +10M -exec rm  {} \\;\n"]}],"source":["sample_example = test_data[1976]\n","\n","import re\n","\n","# Input string\n","input_string = sample_example['text']\n","\n","# Regular expression pattern to extract instruction and code\n","pattern = r'\\[INST\\] Docstring: (.+?) \\[/INST\\] Code: (.+)'\n","\n","# Match the pattern\n","match = re.match(pattern, input_string)\n","\n","# Extract instruction and code\n","if match:\n","    nl_command = match.group(1)\n","    actual_code = match.group(2)\n","    print(\"nl_command =\", nl_command)\n","    print(\"code =\", actual_code)\n","else:\n","    print(\"No match found.\")\n","\n"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.status.busy":"2024-05-25T23:56:02.739906Z","iopub.status.idle":"2024-05-25T23:56:15.202528Z","shell.execute_reply":"2024-05-25T23:56:15.201514Z","shell.execute_reply.started":"2024-05-25T23:56:02.740185Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"]},{"name":"stdout","output_type":"stream","text":["Input NL Command: Find all .mp3 files with more then 10MB and delete them from root directory .\n","Predicted Bash Command: Bash code for Find all .mp3 files with more then 10MB and delete them from root directory .\n","\n","\\begin{code}\n","find / -name \"*.mp3\" -size +10M -exec rm -f {} \\;\n","\\end{code}\n","\n","Comment: I'm not sure if this is a good idea. I'm not sure if it will delete all the files in the directory.\n","\n","Comment: @user1126332, it will delete all the files in the directory.\n","\n","Comment: @user1126332, it will delete all the files in the directory.\n","\n","Comment: @user1126\n","actual_code: find /  -type f -name *.mp3 -size +10M -exec rm  {} \\;\n"]}],"source":["# Run inference\n","input_text = f\"Bash code for {nl_command}\"\n","inputs = og_tokenizer(input_text, return_tensors=\"pt\").to(ft_model.device)\n","outputs = ft_model.generate(**inputs, max_length=150)\n","generated_code = og_tokenizer.decode(outputs[0], skip_special_tokens=True)\n","\n","# Print the results\n","print(\"Input NL Command:\", nl_command)\n","print(\"Predicted Bash Command:\", generated_code)\n","print(f\"actual_code: {actual_code}\")"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-05-26T00:00:42.494154Z","iopub.status.busy":"2024-05-26T00:00:42.493775Z","iopub.status.idle":"2024-05-26T00:00:48.844745Z","shell.execute_reply":"2024-05-26T00:00:48.843550Z","shell.execute_reply.started":"2024-05-26T00:00:42.494125Z"},"trusted":true},"outputs":[],"source":["import torch\n","torch.save(ft_model.state_dict(), 'models/codellama-fine-tuned-nl2bash.pth')"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30699,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.10"}},"nbformat":4,"nbformat_minor":4}
