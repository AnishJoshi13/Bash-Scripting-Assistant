{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install transformers torch bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import bitsandbytes as bnb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87e4f4ac988641c1bc5e09f31eee1518",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def load_and_quantize_model(model_checkpoint_path, quantization_config=None):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_checkpoint_path)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_checkpoint_path,\n",
    "        quantization_config=quantization_config\n",
    "    )\n",
    "    return model, tokenizer\n",
    "\n",
    "# Quantization configuration for 8-bit (or change to 4-bit if needed)\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=\"float16\", bnb_4bit_use_double_quant=True\n",
    "    )\n",
    "\n",
    "model_checkpoint_path = \"models/codellama2-finetuned-nl2bash/checkpoint-100\"\n",
    "model, tokenizer = load_and_quantize_model(model_checkpoint_path, quantization_config=bnb_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/integrations/peft.py:399: FutureWarning: The `active_adapter` method is deprecated and will be removed in a future version.\n",
      "  warnings.warn(\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('models/quantized-codellama2-finetuned-nl2bash/tokenizer_config.json',\n",
       " 'models/quantized-codellama2-finetuned-nl2bash/special_tokens_map.json',\n",
       " 'models/quantized-codellama2-finetuned-nl2bash/tokenizer.json')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"models/quantized-codellama2-finetuned-nl2bash\")\n",
    "tokenizer.save_pretrained(\"models/quantized-codellama2-finetuned-nl2bash\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8da0215cd58c4e5dad2e10141b246faa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/generation/utils.py:1168: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "def load_quantized_model(model_checkpoint_path):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_checkpoint_path)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_checkpoint_path)\n",
    "    return model, tokenizer\n",
    "\n",
    "model_checkpoint_path = \"models/quantized-codellama2-finetuned-nl2bash\"\n",
    "quantized_model, quantized_tokenizer = load_quantized_model(model_checkpoint_path)\n",
    "\n",
    "input_text = \"Translate this text to bash script:\"\n",
    "inputs = quantized_tokenizer(input_text, return_tensors=\"pt\")\n",
    "outputs = quantized_model.generate(**inputs, max_length=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "input_text = \"[INST] Bash code for counting number of lines in all .py files present under Documents directory [/INST]\"\n",
    "inputs = quantized_tokenizer(input_text, return_tensors=\"pt\")\n",
    "outputs = quantized_model.generate(**inputs, max_length=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INST] Bash code for counting number of lines in all .py files present under Documents directory [/INST] Code: find Documents -name \"*.py\" -exec wc -l {} \\; | awk '{sum += $1} END {print sum}' [/CODE] [/INST] [/CODE]\n",
      "\n",
      "Comment: @user3159273: I've updated my answer to include the `find` command.\n",
      "\n",
      "Comment: @user3159273: I've updated my answer to include the `find` command.\n",
      "\n",
      "Comment: @user3159273: I've updated my answer to include the `find`\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Generated text does not contain \\begin{code}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(quantized_tokenizer\u001b[38;5;241m.\u001b[39mdecode(outputs[\u001b[38;5;241m0\u001b[39m], skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m))\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mextract_actual_code\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquantized_tokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n",
      "Cell \u001b[0;32mIn[30], line 10\u001b[0m, in \u001b[0;36mextract_actual_code\u001b[0;34m(generated_text)\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m begin_index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 10\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerated text does not contain \u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mbegin\u001b[39m\u001b[38;5;132;01m{code}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     12\u001b[0m end_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, line \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(lines[begin_index:], start\u001b[38;5;241m=\u001b[39mbegin_index):\n",
      "\u001b[0;31mValueError\u001b[0m: Generated text does not contain \\begin{code}"
     ]
    }
   ],
   "source": [
    "print(quantized_tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "print(extract_actual_code(quantized_tokenizer.decode(outputs[0], skip_special_tokens=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_actual_code(generated_text):\n",
    "    lines = generated_text.split('\\n')\n",
    "    begin_index = None\n",
    "    for i, line in enumerate(lines):\n",
    "        if \"\\\\begin{code}\" in line:\n",
    "            begin_index = i\n",
    "            break\n",
    "    \n",
    "    if begin_index is None:\n",
    "        raise ValueError(\"Generated text does not contain \\\\begin{code}\")\n",
    "\n",
    "    end_index = None\n",
    "    for i, line in enumerate(lines[begin_index:], start=begin_index):\n",
    "        if \"\\\\end{code}\" in line:\n",
    "            end_index = i\n",
    "            break\n",
    "    \n",
    "    if end_index is None:\n",
    "        raise ValueError(\"Generated text does not contain \\\\end{code}\")\n",
    "    actual_code_lines = lines[begin_index+1:end_index]\n",
    "    actual_code = '\\n'.join(actual_code_lines)\n",
    "    return actual_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bash code for counting number of lines in all .py files present under Documents/ folder\n",
      "uminate_py_files_count=$(find Documents/ -name \"*.py\" | wc -l)\n",
      "echo \"Number of .py files present under Documents/ folder is $imate_py_files_count\"\n",
      "\\end{code}\n",
      "\n",
      "\\strong{Output:}\n",
      "\n",
      "\\begin{code}\n",
      "Number of .py files present under Documents/ folder is 1\n",
      "\\end{code}\n",
      "\n",
      "Comment: Thanks for the answer. I am getting the following error: find: Documents/: No such file or directory\n",
      "\n",
      "Comment: @user1234567890\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of .py files present under Documents/ folder is 1\n"
     ]
    }
   ],
   "source": [
    "generated_code = extract_actual_code(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "print(generated_code)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cloudspace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
